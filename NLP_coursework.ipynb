{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWgmD/ajgqKUgmdpRH1UA9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adewale-codes/NLP/blob/main/NLP_coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing required libraries"
      ],
      "metadata": {
        "id": "waq-Wxw5FMir"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpooad3zFHz_"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing required libraries"
      ],
      "metadata": {
        "id": "JlB8b-2KFYXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "g2Qwi1S9FSwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataset"
      ],
      "metadata": {
        "id": "9TE01t5mFkxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/NLP/Reviews.csv\")\n",
        "print(\"Dataset columns:\", df.columns)\n",
        "print(\"First few rows:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "ODlgNhIwFceO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the score column as the rating column"
      ],
      "metadata": {
        "id": "Y4WBc_ZtGO30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rating_column = 'Score'"
      ],
      "metadata": {
        "id": "fXmBsjz-GMZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapping the score column to sentiment lables setting 0 as negative, 1 as neutral and 2 as positive"
      ],
      "metadata": {
        "id": "rAnGbNQuGatO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_sentiment(score):\n",
        "    if score >= 4:\n",
        "        return 2\n",
        "    elif score <= 2:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "-6u5WnbeGTdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a new column called label using the mapping function"
      ],
      "metadata": {
        "id": "QTkG4KPfHFGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'] = df[rating_column].apply(map_sentiment)"
      ],
      "metadata": {
        "id": "6VFxImPzG5Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropped rows with missing review text. The column name is called Text"
      ],
      "metadata": {
        "id": "M2unZBSHHTie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['Text'])"
      ],
      "metadata": {
        "id": "fhZgkvP5HPV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset into training and testing sets"
      ],
      "metadata": {
        "id": "99rjbaUCHmbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "43Oj2pfFHidF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converted the pandas DataFrames to Hugging Face Datasets"
      ],
      "metadata": {
        "id": "5jMuOd3CH10p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
        "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))"
      ],
      "metadata": {
        "id": "Cptx0tNqHvcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialized a BERT tokenizer"
      ],
      "metadata": {
        "id": "jeCVpp_hIHoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "37-W9oJXIEZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenized the dataset using the column called Text"
      ],
      "metadata": {
        "id": "AVGYCWy0IQ5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"Text\"], padding=\"max_length\", truncation=True, max_length=128)"
      ],
      "metadata": {
        "id": "cq4M19HQINmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "kTI5bd9EIbYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the format for PyTorch tensors"
      ],
      "metadata": {
        "id": "0VrL2ylQIktq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
      ],
      "metadata": {
        "id": "KSMmhqywIeX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a pre-trained BERT model for sequence classification with 3 labels negative, neutral, positive"
      ],
      "metadata": {
        "id": "tJigMjaZI1g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
      ],
      "metadata": {
        "id": "GAduM8hRIv-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the evaluation metric which is accuracy using evaluate.load"
      ],
      "metadata": {
        "id": "9MChFp5tJhpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_metric = evaluate.load(\"accuracy\")"
      ],
      "metadata": {
        "id": "KYn2KH5OJMYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy_metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "nFNVD4uZJpaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defined training arguments"
      ],
      "metadata": {
        "id": "0Bj9vuRIJ1n-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=[],\n",
        ")"
      ],
      "metadata": {
        "id": "QPwTf5cYJyAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialized the Trainer"
      ],
      "metadata": {
        "id": "pZljCK3AKK7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "5sRv-y4IKG-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine tuning the model"
      ],
      "metadata": {
        "id": "Jy-MFCwdKP6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "B6cR5vcjKPPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the model on the test dataset"
      ],
      "metadata": {
        "id": "t2ouhOQ-Kcar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = trainer.evaluate()\n",
        "print(\"Evaluation results:\", eval_result)"
      ],
      "metadata": {
        "id": "CteOB2iXKWa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tesing the model"
      ],
      "metadata": {
        "id": "CW3-yxHVKmqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "example_review = \"I absolutely hate this product, it below all my expectations!\"\n",
        "inputs = tokenizer(example_review, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "outputs = model(**inputs)\n",
        "\n",
        "predicted_class = int(np.argmax(outputs.logits.detach().cpu().numpy(), axis=-1)[0])\n",
        "\n",
        "sentiment_mapping = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "print(f\"Review: {example_review}\")\n",
        "print(f\"Predicted Sentiment: {sentiment_mapping[predicted_class]}\")"
      ],
      "metadata": {
        "id": "GuWgADbjKmNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Since the model took so long to run i saved the checkpoints so rather than running from beginning you can run from the third epoch and save time to. To do that i added these codes below"
      ],
      "metadata": {
        "id": "QmnrXVhGLMDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the libraries"
      ],
      "metadata": {
        "id": "PbxHaAvEV7y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import torch"
      ],
      "metadata": {
        "id": "ypaEPFqCLHli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataset"
      ],
      "metadata": {
        "id": "XJLQwdcvWDIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/NLP/Reviews.csv\")\n",
        "print(\"Dataset columns:\", df.columns)\n",
        "print(\"First few rows:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "RYw4-kyzV-w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the score column as the rating column"
      ],
      "metadata": {
        "id": "0TBsBT46XI5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rating_column = 'Score'"
      ],
      "metadata": {
        "id": "VUv2mRrbXIdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapping the score column to sentiment lables setting 0 as negative, 1 as neutral and 2 as positive"
      ],
      "metadata": {
        "id": "SR90DtH8W_W7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_sentiment(score):\n",
        "    if score >= 4:\n",
        "        return 2\n",
        "    elif score <= 2:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "jojOSPXFXEbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the dataset into training and testing sets"
      ],
      "metadata": {
        "id": "6BkS-nJOXWgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
        "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))"
      ],
      "metadata": {
        "id": "a42u-2icXZAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization and Formatting"
      ],
      "metadata": {
        "id": "8_igEl8jXd5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "ZX_SbJUqXhAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"Text\"], padding=\"max_length\", truncation=True, max_length=128)"
      ],
      "metadata": {
        "id": "eqE7BvnnXmIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
      ],
      "metadata": {
        "id": "g-c_yJB0XqKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model and Trainer Setup"
      ],
      "metadata": {
        "id": "l-iVbJ9EXvQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
      ],
      "metadata": {
        "id": "IbTqHp-WXzdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defined evaluation metric using evaluate.load"
      ],
      "metadata": {
        "id": "-Ds-FIsyX3ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy_metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "ptMm7b12YXQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=[],\n",
        ")"
      ],
      "metadata": {
        "id": "eKjsVIE0YaHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "e0hacm36Yhbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkpoint path"
      ],
      "metadata": {
        "id": "gIwlyxM8Yo90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/results/checkpoint-85269\"\n",
        "trainer.train(resume_from_checkpoint=checkpoint_path)"
      ],
      "metadata": {
        "id": "Pi82ojLYYoXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating model after resuming training"
      ],
      "metadata": {
        "id": "ImJwZo_HYwb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = trainer.evaluate()\n",
        "print(\"Evaluation results:\", eval_result)"
      ],
      "metadata": {
        "id": "BGXuoaWGYv7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model from a checkpoint without further training"
      ],
      "metadata": {
        "id": "o7NLEZZiZBH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(checkpoint_path)"
      ],
      "metadata": {
        "id": "JqAH6DR0Y8bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the model"
      ],
      "metadata": {
        "id": "IvZGnt5WZJJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_review = \"I absolutely hate this product, it below all my expectations!\"\n",
        "inputs = tokenizer(example_review, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "outputs = model(**inputs)\n",
        "predicted_class = int(np.argmax(outputs.logits.detach().cpu().numpy(), axis=-1)[0])\n",
        "sentiment_mapping = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "print(f\"Review: {example_review}\")\n",
        "print(f\"Predicted Sentiment: {sentiment_mapping[predicted_class]}\")"
      ],
      "metadata": {
        "id": "dRg3gKN-ZLRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the model to create backend"
      ],
      "metadata": {
        "id": "j9WjfrKmZQZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./saved_model\")\n",
        "\n",
        "tokenizer.save_pretrained(\"./saved_model\")\n"
      ],
      "metadata": {
        "id": "gQWW10F6ZURM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}